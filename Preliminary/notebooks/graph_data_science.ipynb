{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Time to do some data science!!!\n",
        "\n",
        "Note that if you have pre-populated your database using the files in `json_files`, you can just start up with this notebook.\n",
        "\n",
        "If you are running this on Google Colab, run the following cells to import the necessary packages."
      ],
      "metadata": {
        "id": "mU6XUg2rnIvh"
      },
      "id": "mU6XUg2rnIvh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install py2neo"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py2neo\n",
            "  Downloading py2neo-2021.2.4-py2.py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from py2neo) (2026.1.4)\n",
            "Collecting interchange~=2021.0.4 (from py2neo)\n",
            "  Downloading interchange-2021.0.4-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting monotonic (from py2neo)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from py2neo) (26.0)\n",
            "Collecting pansi>=2020.7.3 (from py2neo)\n",
            "  Downloading pansi-2024.11.0-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: pygments>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from py2neo) (2.19.2)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from py2neo) (1.17.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from py2neo) (2.5.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from interchange~=2021.0.4->py2neo) (2025.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from pansi>=2020.7.3->py2neo) (11.3.0)\n",
            "Downloading py2neo-2021.2.4-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interchange-2021.0.4-py2.py3-none-any.whl (28 kB)\n",
            "Downloading pansi-2024.11.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Installing collected packages: monotonic, pansi, interchange, py2neo\n",
            "Successfully installed interchange-2021.0.4 monotonic-1.6 pansi-2024.11.0 py2neo-2021.2.4\n"
          ]
        }
      ],
      "metadata": {
        "id": "4F09pve0nIvk",
        "outputId": "dc21f157-0653-4032-9a6c-ee1d8a5641ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4F09pve0nIvk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import json\n",
        "import re\n",
        "import urllib\n",
        "from pprint import pprint\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
        "from py2neo.bulk import merge_nodes\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns"
      ],
      "outputs": [],
      "metadata": {
        "id": "tYJI3P3knIvl"
      },
      "id": "tYJI3P3knIvl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# If you are using a Docker container for your DB, use the uncommented line.\n",
        "# graph = Graph(\"bolt://some_ip_address:7687\", name=\"neo4j\", password=\"some_password\")\n",
        "\n",
        "uri = ''\n",
        "user = 'neo4j'\n",
        "pwd = ''\n",
        "\n",
        "graph = Graph(uri, auth=('neo4j', pwd))\n",
        "nodes_matcher = NodeMatcher(graph)"
      ],
      "outputs": [],
      "metadata": {
        "id": "cpmbFgwynIvl"
      },
      "id": "cpmbFgwynIvl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's try some more entity disambiguation\n",
        "\n",
        "Recall in the previous notebook that we looked at the cosine similarity of word vectors.  Instead, now let's look at the overlap of the relationships between our starting node, 'oh bah mə', and  'barack hussein obama ii'.  We might expect if there was strong similarity that we would see a lot of relationship overlap.  "
      ],
      "metadata": {
        "id": "2h080pganIvl"
      },
      "id": "2h080pganIvl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pbo_ls = []\n",
        "pbo = graph.run('MATCH (n:Node {name: \"president barack obama\"})--(m) RETURN DISTINCT m.name')\n",
        "for record in pbo:\n",
        "    pbo_ls.append(record[0])\n",
        "print('Total number of connected nodes: ', len(pbo_ls))\n",
        "pbo_ls"
      ],
      "outputs": [],
      "metadata": {
        "id": "0w0aAlxXnIvm"
      },
      "id": "0w0aAlxXnIvm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "obm_ls = []\n",
        "obm = graph.run('MATCH (n:Node {name: \"oh bah mə\"})--(m) RETURN DISTINCT m.name')\n",
        "for record in obm:\n",
        "    obm_ls.append(record[0])\n",
        "print('Total number of connected nodes: ', len(obm_ls))"
      ],
      "outputs": [],
      "metadata": {
        "id": "nAZ68-JcnIvm"
      },
      "id": "nAZ68-JcnIvm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pbo_set = set(pbo_ls)\n",
        "obm_set = set(obm_ls)\n",
        "if (pbo_set & obm_set):\n",
        "    print('Number of overlapping elements: ', len(pbo_set & obm_set))\n",
        "    print('Percent of overlapping elements: ', len(pbo_set & obm_set)/len(pbo_ls))\n",
        "    print(pbo_set & obm_set)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9uFUJZkSnIvm"
      },
      "id": "9uFUJZkSnIvm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation\n",
        "\n",
        "So we can see that 100% of the nodes connected to 'barack hussein obama ii' are in the connected node list of 'oh bah mə'.  This is a strong indicator that the former might be the same entity as the later.\n",
        "\n",
        "# Now let's connect to the graph and do some ML\n",
        "\n",
        "Here we are going to take advantage of the ability to both run Cypher queries in `py2neo` as well as to write the results to a Pandas DataFrame."
      ],
      "metadata": {
        "id": "xfmneHd8nIvn"
      },
      "id": "xfmneHd8nIvn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = graph.run('MATCH (n:Node) RETURN n.name, n.node_labels, n.pptu_person, n.pptu_place, n.pptu_thing, n.pptu_unknown, n.word_vec, n.n2v_all_nodes').to_data_frame()\n",
        "df.columns = ['name', 'node_labels', 'pptu_person', 'pptu_place',\n",
        "              'pptu_thing', 'pptu_unknown', 'word_vec', 'n2v_all_nodes']\n",
        "df2 = df.fillna(0)\n",
        "df2.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "DUBZMK1NnIvn"
      },
      "id": "DUBZMK1NnIvn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a variety of `X` variables for the model (in the style of `scikit-learn`)..."
      ],
      "metadata": {
        "id": "H46hOMStnIvn"
      },
      "id": "H46hOMStnIvn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def create_X(df2):\n",
        "\n",
        "    word_vec_ls = df2['word_vec'].to_list()\n",
        "    word_vec_arr = np.array([np.array(x) if x != 0 else np.zeros(300).tolist() for x in word_vec_ls], dtype=object)\n",
        "\n",
        "    n2v_an_ls = df2['n2v_all_nodes'].to_list()\n",
        "    n2v_arr = np.array([np.array(x) for x in n2v_an_ls], dtype=object)\n",
        "\n",
        "    print(word_vec_arr.shape, n2v_arr.shape)\n",
        "\n",
        "    return word_vec_arr, n2v_arr"
      ],
      "outputs": [],
      "metadata": {
        "id": "4C7Dq6nBnIvn"
      },
      "id": "4C7Dq6nBnIvn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_word_vec, X_all_nodes = create_X(df2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "RpuiU_oEnIvo"
      },
      "id": "RpuiU_oEnIvo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Out of convenience...\n",
        "\n",
        "...we are creating this function that will run a support vector machine classifier to see how well the different embeddings do at predicting different labels.  We do note that this is a multi-label problem (Person, Place, Thing, Unknown), but for simplicity we are going to evaluate the prediction against single labels within that dataset.  The interested reader is encouraged to try more sophisticated models that can handle the multi-label problem better.\n",
        "\n",
        "We also show below that each of the classes is significantly imbalanced.  The interested participant is encourage to experiment with balancing the classes to see how this impacts the overall accuracy."
      ],
      "metadata": {
        "id": "rtqxOrRbnIvo"
      },
      "id": "rtqxOrRbnIvo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def modeler(df, column_name, X, k_folds=5, model='linear', show_matrix=True):\n",
        "\n",
        "    y = df[column_name].fillna(0.0).to_numpy()\n",
        "    acc_scores = []\n",
        "\n",
        "    pos = np.count_nonzero(y == 1.0)\n",
        "    neg = y.shape[0] - pos\n",
        "    print('Number of positive: ', pos, ' Number of negative: ', neg)\n",
        "\n",
        "    for i in range(0, k_folds):\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_word_vec, y, test_size=0.25)\n",
        "        clf = svm.SVC(kernel='linear')\n",
        "        clf.fit(X_train, y_train)\n",
        "        pred = clf.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(pred, y_test)\n",
        "        acc_scores.append(acc)\n",
        "\n",
        "    print('Accuracy scores: ', acc_scores)\n",
        "    print('Mean accuracy: ', np.mean(acc_scores))\n",
        "\n",
        "    if show_matrix:\n",
        "        matrix = plot_confusion_matrix(clf, X_test, y_test, cmap=plt.cm.Blues, normalize='true')\n",
        "        plt.show(matrix)\n",
        "        plt.show()\n",
        "\n",
        "    return"
      ],
      "outputs": [],
      "metadata": {
        "id": "QSzwZK6MnIvo"
      },
      "id": "QSzwZK6MnIvo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "modeler(df2, 'pptu_person', X_word_vec)"
      ],
      "outputs": [],
      "metadata": {
        "id": "hBSUr1uSnIvo"
      },
      "id": "hBSUr1uSnIvo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "modeler(df2, 'pptu_person', X_all_nodes)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZpLaAxqjnIvo"
      },
      "id": "ZpLaAxqjnIvo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "modeler(df2, 'pptu_place', X_word_vec)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Q7lmARxnnIvp"
      },
      "id": "Q7lmARxnnIvp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "modeler(df2, 'pptu_place', X_all_nodes)"
      ],
      "outputs": [],
      "metadata": {
        "id": "I53LIl_AnIvp"
      },
      "id": "I53LIl_AnIvp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "modeler(df2, 'pptu_thing', X_word_vec)"
      ],
      "outputs": [],
      "metadata": {
        "id": "OEW3R3uWnIvp"
      },
      "id": "OEW3R3uWnIvp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "modeler(df, 'pptu_thing', X_all_nodes)"
      ],
      "outputs": [],
      "metadata": {
        "id": "NafHZwuxnIvp"
      },
      "id": "NafHZwuxnIvp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next steps\n",
        "\n",
        "There are so many things that you can try from here!  Some of the things I might consider would be:\n",
        "\n",
        "- Take time to tune the hyperparameters.  This can be done for:\n",
        "  - The spacy word embeddings\n",
        "  - The graph embeddings\n",
        "  - The ML model\n",
        "- Trying more sophisticated embedding approaches, such as GraphSAGE that takes into account the node properties.\n",
        "- Explore different embeddings.  Here we used the spacy word vectors to create embeddings for the nodes, but there are many, many more ways to create vectors that could be used for training the ML models!  Get creative!\n",
        "- Work the class imbalance problem.\n",
        "- This graph is quite small in reality.  Work on growing the graph by adding more layers to it via either Wikipedia or the Google Knowledge Graph.  As the graph gets larger, we might expect that the graph embeddings approaches will start to really shine beyond the word embeddings.\n",
        "\n"
      ],
      "metadata": {
        "id": "lQ-7gj5tnIvp"
      },
      "id": "lQ-7gj5tnIvp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "BwUpesivnIvp"
      },
      "id": "BwUpesivnIvp"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}